# Quick Reference - Generic Framework

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå…¥é—¨

### åˆ›å»ºè‡ªå®šä¹‰ Agent åªéœ€ 3 æ­¥

#### 1ï¸âƒ£ å®šä¹‰ Task

```python
from qwen3_agent.core.framework import BaseTask

class MyTask(BaseTask):
    question: str
    answer: str
    
    def get_query(self) -> str:
        return self.question
    
    def get_ground_truth(self):
        return self.answer
```

#### 2ï¸âƒ£ å®ç° Agent

```python
from qwen3_agent.core.framework import BaseAgent, ActionResult

class MyAgent(BaseAgent):
    def get_system_prompt(self, task):
        return f"Solve: {task.get_query()}"
    
    def get_tools_schema(self):
        return [{
            "type": "function",
            "function": {
                "name": "solve",
                "parameters": {...}
            }
        }]
    
    def execute_action(self, tool_name, tool_args, task):
        if tool_name == "solve":
            result = do_something(**tool_args)
            return ActionResult(success=True, data=result)
        return ActionResult(success=False, error="Unknown tool")
    
    def is_terminal_action(self, tool_name):
        return tool_name == "answer"
```

#### 3ï¸âƒ£ å®ç° Evaluator

```python
from dataclasses import dataclass
from qwen3_agent.core.framework import BaseEvaluator, BaseRubric

@dataclass
class MyRubric(BaseRubric):
    correct: bool = False

class MyEvaluator(BaseEvaluator[MyRubric]):
    def create_rubric(self):
        return MyRubric()
    
    async def evaluate_trajectory(self, traj, task, rubric):
        return 1.0 if rubric.correct else 0.0
    
    def on_action_executed(self, rubric, tool_name, tool_args, result, task):
        if tool_name == "answer":
            rubric.correct = (result == task.get_ground_truth())
```

### ä½¿ç”¨ä½ çš„ Agent

```python
from qwen3_agent.core.framework import LLMInference, generic_rollout

# åˆ›å»ºç»„ä»¶
task = MyTask(id="1", question="...", answer="...")
agent = MyAgent()
evaluator = MyEvaluator()
llm = LLMInference("openai/gpt-4o-mini", {})

# æ‰§è¡Œ
trajectory = await generic_rollout(
    llm=llm,
    task=task,
    agent=agent,
    evaluator=evaluator,
    max_turns=10,
)

print(f"Reward: {trajectory.reward}")
```

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶é€ŸæŸ¥

### BaseTask
| æ–¹æ³• | å¿…é¡»å®ç° | è¯´æ˜ |
|------|---------|------|
| `get_query()` | âœ… | è¿”å›ç»™ Agent çš„é—®é¢˜ |
| `get_ground_truth()` | âœ… | è¿”å›æ­£ç¡®ç­”æ¡ˆ |
| `get_context()` | âŒ | è¿”å›é¢å¤–ä¸Šä¸‹æ–‡ |

### BaseAgent
| æ–¹æ³• | å¿…é¡»å®ç° | è¯´æ˜ |
|------|---------|------|
| `get_system_prompt(task)` | âœ… | ç”Ÿæˆç³»ç»Ÿæç¤º |
| `get_tools_schema()` | âœ… | è¿”å›å·¥å…·åˆ—è¡¨ |
| `execute_action(...)` | âœ… | æ‰§è¡Œå·¥å…· |
| `is_terminal_action(name)` | âœ… | åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢ |
| `parse_action(msg, native)` | âŒ | è§£æå“åº”ï¼ˆæœ‰é»˜è®¤å®ç°ï¼‰|

### BaseEvaluator
| æ–¹æ³• | å¿…é¡»å®ç° | è¯´æ˜ |
|------|---------|------|
| `create_rubric()` | âœ… | åˆ›å»ºè¯„ä¼°æŒ‡æ ‡ |
| `evaluate_trajectory(...)` | âœ… | è®¡ç®—æœ€ç»ˆå¥–åŠ± |
| `on_action_executed(...)` | âœ… | æ›´æ–°è¯„ä¼°æŒ‡æ ‡ |
| `on_parsing_error(...)` | âŒ | å¤„ç†é”™è¯¯ |

### LLMInference
```python
# ART æ¨¡å‹
llm = LLMInference(art_model)

# å¤–éƒ¨æ¨¡å‹
llm = LLMInference("openai/gpt-4o", {"api_key": "..."})

# è°ƒç”¨
response = await llm.complete(messages, tools=tools)
```

## ğŸ’¡ å¸¸ç”¨æ¨¡å¼

### 1. å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰

```python
def get_tools_schema(self):
    return [{
        "type": "function",
        "function": {
            "name": "tool_name",
            "description": "Tool description",
            "parameters": {
                "type": "object",
                "properties": {
                    "arg1": {"type": "string", "description": "..."},
                    "arg2": {"type": "number"}
                },
                "required": ["arg1"]
            }
        }
    }]
```

### 2. å·¥å…·æ‰§è¡Œ

```python
def execute_action(self, tool_name, tool_args, task):
    try:
        if tool_name == "my_tool":
            result = my_implementation(**tool_args)
            return ActionResult(success=True, data=result)
        else:
            return ActionResult(success=False, error="Unknown tool")
    except Exception as e:
        return ActionResult(success=False, error=str(e))
```

### 3. è¯„ä¼°æŒ‡æ ‡æ›´æ–°

```python
def on_action_executed(self, rubric, tool_name, tool_args, result, task):
    # è¿½è¸ªæ“ä½œæ¬¡æ•°
    rubric.num_operations += 1
    
    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°å…³é”®ä¿¡æ¯
    if tool_name == "search" and "important_data" in result:
        rubric.found_data = True
    
    # æ£€æŸ¥æœ€ç»ˆç­”æ¡ˆ
    if tool_name == "answer":
        rubric.answer_correct = (result == task.get_ground_truth())
```

### 4. å¥–åŠ±è®¡ç®—

```python
async def evaluate_trajectory(self, traj, task, rubric):
    # ç®€å•å¥–åŠ±
    if rubric.correct:
        return 1.0
    else:
        return 0.0
    
    # å¤æ‚å¥–åŠ±ï¼ˆéƒ¨åˆ†åˆ†ï¼‰
    reward = 0.0
    if rubric.correct:
        reward += 1.0
    if rubric.found_data:
        reward += 0.3
    if rubric.efficient:
        reward += 0.2
    return reward
```

## ğŸ”„ å‘åå…¼å®¹

ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹ï¼š

```python
# è‡ªåŠ¨ä½¿ç”¨æ–°æ¡†æ¶
from qwen3_agent.rollout_compat import rollout

trajectory = await rollout(model, scenario)
```

## ğŸ“š å®Œæ•´æ–‡æ¡£

- [FRAMEWORK.md](FRAMEWORK.md) - è¯¦ç»†æ–‡æ¡£
- [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) - è¿ç§»æŒ‡å—
- [examples/simple_math_agent.py](examples/simple_math_agent.py) - å®Œæ•´ç¤ºä¾‹

## ğŸ§ª æµ‹è¯•

```bash
# æµ‹è¯•æ–°æ¡†æ¶
uv run python test_framework.py

# æµ‹è¯•ç¤ºä¾‹ Agent
uv run python examples/simple_math_agent.py
```

---

**æç¤º**: æŸ¥çœ‹ `examples/simple_math_agent.py` è·å–å®Œæ•´çš„å¯è¿è¡Œç¤ºä¾‹ï¼

