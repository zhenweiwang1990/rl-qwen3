# =============================================================================
# API Keys
# =============================================================================

# OpenAI API Key (required for GPT-4o as judge in evaluation)
OPENAI_API_KEY=your_openai_api_key_here

# Weights & Biases (for training monitoring and logging)
# Get your key from https://wandb.ai/settings
WANDB_API_KEY=your_wandb_api_key_here
WANDB_MODE=online  # Set to 'disabled' to turn off WandB
WANDB_PROJECT=qwen3-email-agent
# WANDB_ENTITY=your_wandb_username_or_team

# =============================================================================
# AWS S3 Configuration (for checkpoint backup)
# =============================================================================

# S3 bucket name for storing model checkpoints and training logs
BACKUP_BUCKET=your-s3-bucket-name

# AWS credentials
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-east-1

# =============================================================================
# Model Configuration
# =============================================================================

# Model Configuration
# For training
MODEL_NAME=OpenPipe/Qwen3-14B-Instruct
RUN_ID=001
MAX_TURNS=10
MAX_TOKENS=2048

# =============================================================================
# Training Configuration
# =============================================================================

# Number of trajectory samples per scenario
TRAJECTORIES_PER_GROUP=6

# Number of scenarios to process per training step
GROUPS_PER_STEP=8

# Learning rate for gradient updates
LEARNING_RATE=1.2e-5

# Evaluate every N steps
EVAL_STEPS=30

# Validation set size for evaluation
VAL_SET_SIZE=100

# Total training dataset size
TRAINING_DATASET_SIZE=4000

# Number of training epochs
NUM_EPOCHS=4

# =============================================================================
# System Configuration
# =============================================================================

# Enable verbose logging
VERBOSE=false

# Device to use: cuda, mps, or cpu (auto-detected if not set)
# DEVICE=cuda
# TORCH_DEVICE=cuda:0  # or mps for Apple Silicon

# =============================================================================
# Rollout Configuration
# =============================================================================

# Maximum concurrent rollout tasks
ROLLOUT_CONCURRENCY=10

# Timeout for LLM API calls (seconds)
LITELLM_TIMEOUT=60

# Maximum retries for failed LLM API calls
LITELLM_MAX_RETRIES=0

# =============================================================================
# Benchmark Configuration
# =============================================================================

# Run benchmarks sequentially (useful for debugging)
BENCH_SEQUENTIAL=false

# Number of test scenarios for benchmarking
TEST_SET_SIZE=100

# =============================================================================
# Optional: Fast Hugging Face Downloads
# =============================================================================

# Enable fast HF transfers (requires hf-transfer package)
HF_HUB_ENABLE_HF_TRANSFER=1
PROFILE_MCP_BASE_URL="http://localhost:4111/api/mcp/profile-mcp-server/mcp"